stages:
  - package
  - container_scan
  # - send_email
  - deploy

#code quality checking
# sonar scanning:

before_script:
  - apk add --no-cache gnupg
  - echo "$ENV_PASS_PHRASE" | gpg --batch --yes --passphrase-fd 0 --decrypt --output .env .env.gpg

######building docker image
package_backend:
  stage: package
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  tags:
    - docker
  before_script:
    - apk update && apk add --no-cache nodejs npm
    - echo "$CI_GITLAB_TOKEN" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"
  script:
    - npm install
    - npm run build
    - docker build -t $CI_REGISTRY_IMAGE .
    - docker push $CI_REGISTRY_IMAGE
    - docker tag $CI_REGISTRY_IMAGE 680866457108.dkr.ecr.us-east-1.amazonaws.com/library:latest

#container Scanning vulnerbility checking for backend image(Trivy)
container_scanning:
  stage: container_scan
  image:
    name: docker.io/aquasec/trivy:latest
    entrypoint: [""]
  variables:
    GIT_STRATEGY: none
    TRIVY_USERNAME: "$CI_REGISTRY_USER"
    TRIVY_PASSWORD: "$CI_GITLAB_TOKEN"
    TRIVY_AUTH_URL: "$CI_REGISTRY"
  before_script:
    - echo $CI_JOB_URL
    - echo $CI_PROJECT_DIR
  script:
    - trivy --version
    # cache cleanup is needed when scanning images with the same tags, it does not remove the database
    - time trivy image --clear-cache
    - trivy image --format json -o container-scanning-report.json --severity HIGH --exit-code 1 $CI_REGISTRY_IMAGE || echo "FAILED" > container-scan-result.txt
    - apk add --no-cache nodejs npm
    - npm install mongodb
    - npm install dotenv-webpack@8.0.1
    - node reports/upload.js container-scanning-report.json
  artifacts:
    when: always
    paths:
      - container-scanning-report.json
      - container-scan-result.txt
  tags:
    - docker
  dependencies:
    - package_backend

#######deploying backend if no vulnerbilites were found
deploy_backend:
  stage: deploy
  before_script:
    - apk add aws-cli
  script:
    - aws --version
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 680866457108.dkr.ecr.us-east-1.amazonaws.com
    - docker push 680866457108.dkr.ecr.us-east-1.amazonaws.com/library:latest
    - docker logout
    - chmod +x ./deploy/ec2/*
    - ./deploy/ec2/deploy.sh
  dependencies:
    - container_scanning
  tags:
    - docker

#########sending email to the developement team if any high vulnerbilities found in container_scanning stage
# send_email:
#   stage: send_email
#   script:
#     - if [ -f "container-scan-result.txt" ]; then ./email/send_email_script.sh; fi
#     # if [ -f "$CI_PROJECT_DIR/container-scan-result.txt" ]
#   before_script:
#     - echo $CI_JOB_URL
#     - apk add --no-cache wget
#     - apk add --no-cache mailx
#     - chmod +x ./email/send_email_script.sh
#   dependencies:
#     - container_scanning
#   tags:
#     - docker




# for creating project runners
# https://killercoda.com/rkalluru
# volumes = ["/cache", "/var/run/docker.sock:/var/run/docker.sock", "/builds:/builds"]
# sudo usermod -aG docker root

# commands in deploy_backend:
# - apt-get install wget -y
#     - pwd
#     - ls
#     - chmod +x deploy/ec2/*
#     - deploy/ec2/deploy.sh
# sendEmail:
#   stage: send-email

# registry.gitlab.example.com/my_group/my_project CI_REGISTRY_IMAGE format
# CI_COMMIT_REF_SLUG --> CI_COMMIT_REF_NAME in lowercase, shortened to 63 bytes, and with everything except 0-9 and a-z replaced with -.
# No leading / trailing -. Use in URLs, host names and domain names.

# Frontend:
# react

# backend:
# node
# node js through AWS communication
# to save cost we did not integrate with frontned
# communication happens through nodjs

# Updates are through rest API's for the frontend to be consumed
# AWS keeps on updating frequently every minute through node we have written a process for everyday like we are writting a CRON job to specify when the job should run we have written a service there also in CRON job
# through this we are getting the realtime data for it
# the data is being saved in mongo db and through REST API's we are getting the data and consuming them in the front end.
